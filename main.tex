\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[numbers]{natbib}

\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\subjectto}{subject\ to}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\Exp}{\mathbf{E}}
\DeclareMathOperator{\RewardRegret}{RewardRegret}
\DeclareMathOperator{\EstimationRegret}{EstimationRegret}
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle} % inner product or bilinear map

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}

% Set of real numbers
\newcommand{\R}{\mathbb{R}}

\newcommand{\F}{\mathcal{F}}
\newcommand{\Z}{\mathcal{Z}}

\begin{document}

\title{Partially Linear Bandits}
\author{David P\'al}
\date{January 30, 2026}
\maketitle

\section{Introduction}

We propose several variants of a new sequential decision problem which we call
\emph{partially linear bandit problem}. We do not provide any results (no
algorithms, no theorems). The problem is related to Double Machine
Learning~\cite{Chernozhukov-Chetverikov-Demirer-Duflo-Hansen-Newey-Robins-2024}
and stochastic linear bandit problem~\cite{Abbasi-Yadkori-Pal-Szepesvari-2011},
\cite[Chapter 19]{Lattimore-Szepesvari-2020}.

Partially linear bandit problem is a sequential decision making problem played
over a number of rounds. At the beginning of each round $t$, we are given a
context feature vector $z_t \in \mathcal{Z}$ and a decision set $D_t \subseteq
\R^d$. We need to choose a decision vector $x_t \in D_t$. We then observe a
reward $y_t \in \R$. We assume that $\mathcal{Z}$ is an non-empty set.

There are several variants of the problem that differ in how $z_t, y_t, D_t$
are generated and what is our objective.

\section{Variants}

\subsection{Decision sets $D_t$}

We assume that $D_t$ is a compact subset of a compact set $D \subset \R^d$.
The set $D_t$ can finite or infinite. There are several variants of the problem
depending how the sequence $D_1, D_2, \dots$ can be generated.

\begin{enumerate}
\item $D_t = D$ for all $t=1,2,\dots$.

\item $D_1, D_2, \dots$ is an i.i.d. sequence.

\item $D_1, D_2, \dots$ is an arbitrary sequence chosen before the first round
of the game. That is, the sequence is generated by an oblivious adversary.

\item $D_1, D_2, \dots$ is generated by adaptive adversary. That is, $D_t$ can
depend on $x_1, x_2, \dots, x_{t-1}$, $y_1, y_2, \dots, y_{t-1}$, $z_1, z_2,
\dots z_{t-1}$ and $D_1, D_2, \dots, D_{t-1}$.
\end{enumerate}

\subsection{Context feature vector $z_t$}

There are several variants of the problem depending on how the sequence $z_1,
z_2, \dots$ can be generated.

\begin{enumerate}
\item $z_1, z_2, \dots$ is an i.i.d. sequence.  An interesting special case is
when $(D_1, z_1), (D_2, z_2), \dots,$ is an i.i.d. sequence.

\item $z_1, z_2, \dots$ is an arbitrary sequence chosen before the first round
of the game. That is, the sequence is generated by an oblivious adversary.

\item $z_1, z_2, \dots$ is generated by adaptive adversary. That is, $z_t$ can
depend on $x_1, x_2, \dots, x_{t-1}$, $y_1, y_2, \dots, y_{t-1}$, $z_1, z_2,
\dots z_{t-1}$ and $D_1, D_2, \dots, D_{t-1}$.
\end{enumerate}


\subsection{Reward $y_t$}

Intuitively speaking, $y_t$ depends on both $x_t$ and $z_t$. However, formally
speaking the dependency on $z_t$ might not exist. Formally, we define that
$$
y_t = c_t + \theta_*^T x_t + \epsilon_t
$$
where $\theta_* \in \R^d$ and $\epsilon_1, \epsilon_2, \dots$ is i.i.d.
sequence with zero mean, indepent of $x_1, x_2, \dots$, $z_1, z_2, \dots$,
$D_1, D_2, \dots$, and $c_1, c_2, \dots$. There are several variants how $c_1,
c_2, \dots$ is generated.

\begin{enumerate}
\item The ``realizable'' case is that $c_t = f^*(z_t)$ where $f^*:\Z \to \R$ is
a function that lies in a function class $\mathcal{F}$.

\item The ``stochastic agnostic'' case is that $c_1, c_2, \dots$ is an i.i.d.
sequence. A particular subvariant is that $(D_1, z_1, c_1), (D_2, z_2, c_2),
\dots$ is an i.i.d. sequence.

\item The ``adversarial'' case is that $c_1, c_2, \dots$ is an arbitrary
sequence.  Two subvariants are possible depending on the power of the
adversary. For an oblivious adversary, $c_1, c_2, \dots$ is chosen before the
first round of the game. For an adaptive adversary, $c_t$ can depend on $(x_1,
y_1, z_1, c_1, D_1), (x_2, y_2, z_2, c_2, D_2), \dots, (x_{t-1}, y_{t-1},
z_{t-1}, c_{t-1}, D_{t-1})$.

\end{enumerate}

Note that in the first variant
$$
y_t = f^*(z_t) + \theta_*^T x_t + \epsilon_t
$$
In other words, the reward is a linear function of $x_t$ and, in general, a
non-linear function of $z_t$. This model is called a partially linear model.

\section{Goal of the learner}

There are two possible goals we can have.

\begin{enumerate}

\item Over $T$ rounds, we collect cumulative reward $\sum_{t=1}^T y_t$. Our
goal is collect as much reward as possible.

\item Our goal is to predict $y_t$, for any possible choice of $x_t$ than can
be made. That is, in round $t$, in addition to $x_t$, we have to come up with a
with an affine function
$$
g_t(x) = \widehat{c}_t + \widehat{\theta}_t^T x
$$
In other words, based on $z_t$ and $D_t$ and all the past information from
rounds $1, 2, \dots, t-1$, we have to come up with a intercept $\widehat{c}_t
\in \R$ and a vector $\widehat{\theta}_t \in \R^d$.

The accurracy of the function $g_t$ is measured by the squared loss $\ell(y,
\widehat{y}) = (y - \widehat{y})^2$ for the worst possible choice of $x \in
D_t$. That is, the goal of the learner is to minimize
$$
\max_{x \in D_t} \ell(c_t + \theta_*^T x, g_t(x)) \: .
$$
An alternative goal is to minimize the cumulative loss
$$
\sum_{t=1}^T \max_{x \in D_t} \ell(c_t + \theta_*^T x, g_t(x)) \: .
$$
\end{enumerate}

\section{Regret}

The two goals give rise to different notions of regret.  For the goal of
maximizing the reward, we consider the policy that in round $t$ chooses the
action
$$
x_t^* = \argmax_{x \in D_t} \theta_*^T x
$$
that maximizes the expected reward. \emph{Reward regret} is the difference of
the rewards  with respect to this policy is
$$
\RewardRegret_T = \sum_{t=1}^T \ip{\theta_*}{x_t - x_t^*} \; .
$$

For the goal of accurately estimating $y_t$ for any possible choice of $x_t$,
let us first define the optimal predictor that depends only on $z_t$,
$$
f^*
= \argmin_{f \in \mathcal{F}} \sum_{t=1}^T \ell(c_t, f(z_t))
= \argmin_{f \in \mathcal{F}} \sum_{t=1}^T (c_t - f(z_t))^2 \: .
$$
The estimation regret is
\begin{align*}
\EstimationRegret_T
& = \sum_{t=1}^T \max_{x \in D_t} \ell(c_t + \theta_*^T x,  \widehat{c}_t + \widehat{\theta}_t^T x) - \ell(c_t + \theta_*^T x, f^*(z_t) + \theta_*^T x) \\
& = \sum_{t=1}^T \max_{x \in D_t} \ell(c_t + \theta_*^T x,  \widehat{c}_t + \widehat{\theta}_t^T x) - \ell(c_t, f^*(z_t)) \\
& = \sum_{t=1}^T \max_{x \in D_t} (c_t + \theta_*^T x -  \widehat{c}_t - \widehat{\theta}_t^T x)^2 - (c_t - f^*(z_t))^2 \: .
\end{align*}

\bibliography{bibliography}
\bibliographystyle{chicago}

\end{document}
