\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[numbers]{natbib}

\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\subjectto}{subject\ to}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}

\begin{document}

\title{Partially Linear Bandits}
\author{David P\'al}
\date{January 30, 2026}
\maketitle

\section{Introduction}

We propose several variants of a new sequential decision problem which we call
\emph{partially linear bandit problem}. We do not provide any results (no
algorithms, no theorems).

Partially linear bandit problem is a sequential decision making problem played
over a number of rounds. At the beginning of each round $t$, we are given a
context feature vector $z_t \in \mathcal{Z}$ and a decision set $D_t \subseteq
\R^d$. We need to choose a decision vector $x_t \in D_t$. We then observe a
reward $y_t \in \R$. We assume that $\mathcal{Z}$ is an non-empty set.

There are several variants of the problem that differ in how $z_t, y_t, D_t$
are generated and what is our objective.

\section{Variants}

\subsection{Decision sets $D_t$}

We assume that $D_t$ is a compact subset of a compact set $D \subset \R^d$.
The set $D_t$ can finite or infinite. There are several variants of the problem
depending how the sequence $D_1, D_2, \dots$ can be generated.

\begin{enumerate}
\item $D_t = D$ for all $t=1,2,\dots$.

\item $D_1, D_2, \dots$ is an i.i.d. sequence.

\item $D_1, D_2, \dots$ is an arbitrary sequence chosen before the first round
of the game. That is, the sequence is generated by an oblivious adversary.

\item $D_1, D_2, \dots$ is generated by adaptive adversary. That is, $D_t$ can
depend on $x_1, x_2, \dots, x_{t-1}$, $y_1, y_2, \dots, y_{t-1}$, $z_1, z_2,
\dots z_{t-1}$ and $D_1, D_2, \dots, D_{t-1}$.
\end{enumerate}

\subsection{Context feature vector $z_t$}

There are several variants of the problem depending on how the sequence $z_1,
z_2, \dots$ can be generated.

\begin{enumerate}
\item $z_1, z_2, \dots$ is an i.i.d. sequence.  An interesting special case is
when $(D_1, z_1), (D_2, z_2), \dots,$ is an i.i.d. sequence.

\item $z_1, z_2, \dots$ is an arbitrary sequence chosen before the first round
of the game. That is, the sequence is generated by an oblivious adversary.

\item $z_1, z_2, \dots$ is generated by adaptive adversary. That is, $z_t$ can
depend on $x_1, x_2, \dots, x_{t-1}$, $y_1, y_2, \dots, y_{t-1}$, $z_1, z_2,
\dots z_{t-1}$ and $D_1, D_2, \dots, D_{t-1}$.
\end{enumerate}


\subsection{Reward $y_t$}

Intuitively speaking, $y_t$ depends on both $x_t$ and $z_t$. However, formally
speaking the dependency on $z_t$ might not exist. There are several variants of
the problem depending on the sequence $y_1, y_2, \dots$ can be generated.

\begin{enumerate}
\item The most basic model is the partially linear model $y_t = f(z_t) +
\theta^T x_t + \epsilon_t$ where $f:\mathcal{Z} \to \R$ is a function that
belongs to a class of functions $\mathcal{F}$, and $\theta \in \Theta$ where
$\Theta \subseteq \R^d$, and $\epsilon_1, \epsilon_2, \dots$ is i.i.d.
sequence with zero mean.

\item $y_t = w_t + \theta^T x_t$ where $w_1, w_2, \dots$ is an i.i.d.
sequence. A particular subvariant is to that $(D_1, z_1, w_1), (D_2, z_2, w_2),
\dots$ is i.i.d. sequence. We wish to find a function from a function class
$\mathcal{F}$ such that $f(z_t) \approx w_t$.

\item $y_t = w_t + \theta^T x_t$ where $w_1, w_2, \dots$ is an arbitrary
sequence.  Two variants are possible depending on the power of the adversary,
oblivious vs.  adaptive. If adversary is oblivious, $w_1, w_2, \dots$ is chosen
before the first round of the game. If the adversary is adaptive, $w_t$ can
depend on $x_1, x_2, \dots, x_{t-1}$, $y_1, y_2, \dots, y_{t-1}$, $z_1, z_2,
\dots z_{t-1}$ and $D_1, D_2, \dots, D_{t-1}$.

\end{enumerate}

\subsection{Goal of the learner}

There are two possible goals we can have.

\begin{enumerate}

\item Over $T$ rounds, we collect cumulative reward $\sum_{t=1}^T y_t$. Our
goal is collect as much reward as possible. 

\item Our goal is to predict $y_t$, for any possible choice of $x_t$ than can
be made. That is, in round $t$, we have to come up with a with an affine
function 
$$
g_t(x) = \widehat{c}_t + \widehat{\theta}_t^T x
$$
In other words, based on $z_t$ and $D_t$ and all the past information from
rounds $1, 2, \dots, t-1$, we have to come up with a intercept $\widehat{c}_t
\in \R$ and a vector $\widehat{\theta}_t \in \R^d$.

The accurracy of the function $g_t$ is measured by the squared loss $\ell(y,
\widehat{y}) = (y - \widehat{y})^2$ for the worst possible choice of $x \in
D_t$. That is, the goal of the learner is to minimize 
$$
\max_{x \in D_t} \ell(y_t, g_t(x)) \: .
$$
An alternative goal is to minimize the cumulative loss
$$
\sum_{t=1}^T \max_{x \in D_t} \ell(y_t, g_t(x)) \: .
$$
\end{enumerate}

\nocite{*}
\bibliography{bibliography}
\bibliographystyle{chicago}

\end{document}
